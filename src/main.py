import os
import json
import datetime
from pathlib import Path
from fastapi import FastAPI, HTTPException, UploadFile, File
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse
from pydantic import BaseModel
from dotenv import load_dotenv
from openai import OpenAI

# --- RAG ä¾èµ– ---
import chromadb
from chromadb.utils import embedding_functions
import PyPDF2
import io

load_dotenv()

app = FastAPI()
app.mount("/static", StaticFiles(directory="static"), name="static")

HISTORY_DIR = Path("history")
CHROMA_PATH = "chroma_db"

# ==========================================
# æ ¸å¿ƒæ¨¡å—: RAG å‘é‡æ•°æ®åº“å¼•æ“ (ä¿æŒä¸å˜)
# ==========================================

chroma_client = chromadb.PersistentClient(path=CHROMA_PATH)

# ä½¿ç”¨è½»é‡çº§å¼€æº Embedding æ¨¡å‹
emb_fn = embedding_functions.SentenceTransformerEmbeddingFunction(
    model_name="all-MiniLM-L6-v2"
)

collection = chroma_client.get_or_create_collection(
    name="local_knowledge",
    embedding_function=emb_fn
)


def add_text_to_rag(filename: str, text: str):
    """å°†æ–‡æœ¬åˆ‡ç‰‡å¹¶å­˜å…¥å‘é‡æ•°æ®åº“"""
    chunk_size = 500
    chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]
    if not chunks: return 0

    ids = [f"{filename}_{i}" for i in range(len(chunks))]
    metadatas = [{"source": filename} for _ in range(len(chunks))]

    collection.add(documents=chunks, metadatas=metadatas, ids=ids)
    return len(chunks)


def query_rag_db(query: str, n_results: int = 3):
    """æ£€ç´¢æœ€ç›¸ä¼¼çš„æ–‡æœ¬ï¼Œè¿”å›å­—ç¬¦ä¸²ï¼Œå¦‚æœè·ç¦»å¤ªè¿œå¯ä»¥è¿”å›ç©º"""
    try:
        results = collection.query(query_texts=[query], n_results=n_results)
        docs = results['documents'][0]
        # è¿™é‡Œå¯ä»¥ç›´æ¥è¿”å›ï¼ŒLLM ä¼šåˆ¤æ–­æ˜¯å¦æœ‰ç”¨
        return "\n---\n".join(docs) if docs else ""
    except Exception as e:
        print(f"RAG Search Error: {e}")
        return ""


# ==========================================
# æ•°æ®æ¨¡å‹ä¸ API æ¥å£
# ==========================================

class ChatRequest(BaseModel):
    api_url: str
    api_key: str
    model: str
    messages: list
    session_file: str


class LoadHistoryRequest(BaseModel):
    filepath: str


# --- åŸºç¡€é¡µé¢ ---
@app.get("/")
async def read_index():
    return FileResponse('static/index.html')


@app.get("/api/config")
async def get_config():
    return {
        "api_url": os.getenv("PROXY_BASE_URL", "https://api.openai.com/v1"),
        "api_key": os.getenv("PROXY_API_KEY", ""),
        "model": os.getenv("TARGET_MODEL", "gpt-3.5-turbo")
    }


# --- çŸ¥è¯†åº“ä¸Šä¼ æ¥å£ ---
@app.post("/api/rag/upload")
async def upload_to_rag(file: UploadFile = File(...)):
    try:
        filename = file.filename
        content = await file.read()
        text_content = ""

        if filename.lower().endswith(".pdf"):
            pdf_reader = PyPDF2.PdfReader(io.BytesIO(content))
            for page in pdf_reader.pages:
                t = page.extract_text()
                if t: text_content += t + "\n"
        else:
            text_content = content.decode("utf-8", errors='ignore')

        count = add_text_to_rag(filename, text_content)
        return {"status": "success", "chunks_added": count, "filename": filename}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


# --- èŠå¤©ä¸»æ¥å£ (é‡æ„ä¸ºç»å…¸çš„ Context Injection RAG) ---
@app.post("/api/chat")
async def chat_endpoint(request: ChatRequest):
    try:
        client = OpenAI(base_url=request.api_url, api_key=request.api_key)

        # 1. è·å–ç”¨æˆ·æœ€æ–°çš„é—®é¢˜
        user_query = ""
        for msg in reversed(request.messages):
            if msg['role'] == 'user':
                user_query = msg['content']
                break

        current_messages = list(request.messages)

        # 2. ã€æ ¸å¿ƒä¿®æ”¹ã€‘ç›´æ¥æœç´¢æœ¬åœ°çŸ¥è¯†åº“ (ä¸ç»è¿‡ LLM æ€è€ƒ)
        # åªè¦ç”¨æˆ·å‘äº†æ¶ˆæ¯ï¼Œæˆ‘ä»¬å°±å»æ•°æ®åº“æä¸€ä¸‹çœ‹æœ‰æ²¡æœ‰ç›¸å…³çš„
        if user_query:
            print(f"ğŸ” æ­£åœ¨æ£€ç´¢çŸ¥è¯†åº“: {user_query}")
            context_data = query_rag_db(user_query)

            if context_data:
                print(f"âœ… æ‰¾åˆ°ç›¸å…³èƒŒæ™¯çŸ¥è¯†ï¼Œæ­£åœ¨æ³¨å…¥ Prompt...")
                # æ„é€ ä¸€ä¸ªç³»ç»Ÿæç¤ºè¯ï¼Œæ’å…¥åˆ°ç”¨æˆ·é—®é¢˜ä¹‹å‰
                # å‘Šè¯‰ LLMï¼šè¿™æ˜¯èƒŒæ™¯èµ„æ–™ï¼Œè¯·å‚è€ƒå®ƒ
                rag_system_prompt = {
                    "role": "system",
                    "content": f"ã€å‚è€ƒèµ„æ–™ï¼ˆè¯·ä¼˜å…ˆåŸºäºæ­¤èµ„æ–™å›ç­”ï¼‰ã€‘\n{context_data}\n\nã€ç”¨æˆ·é—®é¢˜ã€‘å¦‚ä¸‹ï¼š"
                }
                # å°†å‚è€ƒèµ„æ–™æ’å…¥åˆ°å€’æ•°ç¬¬äºŒä¸ªä½ç½®ï¼ˆå³æœ€æ–°ç”¨æˆ·æé—®ä¹‹å‰ï¼‰
                # è¿™æ ·å¯ä»¥ä¿è¯ä¸Šä¸‹æ–‡è¿è´¯æ€§
                current_messages.insert(-1, rag_system_prompt)

        # 3. å‘é€ç»™ LLM (æ™®é€šå¯¹è¯æ¨¡å¼ï¼Œæ—  Tool)
        response = client.chat.completions.create(
            model=request.model,
            messages=current_messages
        )

        final_content = response.choices[0].message.content

        # 4. ä¿å­˜å†å²
        # æ³¨æ„ï¼šä¿å­˜å†å²æ—¶ï¼Œæˆ‘ä»¬ä¸ä¿å­˜é‚£ä¸ªä¸´æ—¶çš„â€œå‚è€ƒèµ„æ–™â€system promptï¼Œ
        # å¦åˆ™å†å²è®°å½•ä¼šå˜å¾—éå¸¸è‡ƒè‚¿ã€‚åªä¿å­˜ç”¨æˆ·é—®é¢˜å’Œ AI å›ç­”ã€‚
        new_history = request.messages + [{"role": "assistant", "content": final_content}]
        save_history_to_file(new_history, request.session_file)

        return {"role": "assistant", "content": final_content}

    except Exception as e:
        print(f"âŒ Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# --- è¾…åŠ©å‡½æ•°ï¼šä¿å­˜/åŠ è½½å†å² ---
def save_history_to_file(messages, filename):
    now = datetime.datetime.now()
    date_str = now.strftime("%Y-%m-%d")
    save_dir = HISTORY_DIR / date_str
    save_dir.mkdir(parents=True, exist_ok=True)
    if not filename.endswith('.json'): filename += '.json'
    file_path = save_dir / filename
    with open(file_path, 'w', encoding='utf-8') as f:
        json.dump(messages, f, ensure_ascii=False, indent=2)


@app.get("/api/history/list")
async def list_history():
    if not HISTORY_DIR.exists(): return {}
    result = {}
    for date_dir in sorted(HISTORY_DIR.iterdir(), reverse=True):
        if date_dir.is_dir():
            files = [f.name for f in sorted(date_dir.glob("*.json"), key=os.path.getmtime, reverse=True)]
            if files: result[date_dir.name] = files
    return result


@app.post("/api/history/load")
async def load_history(req: LoadHistoryRequest):
    file_path = HISTORY_DIR / req.filepath
    if not file_path.exists(): raise HTTPException(status_code=404, detail="Not Found")
    with open(file_path, "r", encoding="utf-8") as f: return json.load(f)


@app.post("/api/models")
async def list_models(data: dict):
    try:
        client = OpenAI(base_url=data['api_url'], api_key=data['api_key'])
        models = client.models.list()
        return {"models": sorted([m.id for m in models.data])}
    except Exception as e:
        return {"error": str(e), "models": []}


if __name__ == "__main__":
    import uvicorn

    print("ğŸš€ æç®€ç‰ˆæœ¬åœ° RAG å¯åŠ¨ä¸­...")
    uvicorn.run(app, host="127.0.0.1", port=8000)